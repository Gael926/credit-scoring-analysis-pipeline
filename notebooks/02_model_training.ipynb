{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_md",
            "metadata": {},
            "source": [
                "# 02. Entraînement et Sélection du Modèle Final\n",
                "\n",
                "Ce notebook implémente le workflow complet validé :\n",
                "1. **Entraînement Initial** : Comparer 4 modèles de base (Dummy, LogReg, RF, XGB) sur les datasets V1 et V2 pour choisir le meilleur dataset.\n",
                "2. **Sélection Dataset & Split** : On fixe le split (Train/Val/Test) du meilleur dataset identifié.\n",
                "3. **Optimisation LightGBM** : On optimise LightGBM uniquement sur ce dataset.\n",
                "4. **Cross-Validation & Construction Ensemble** : On entraîne 5 modèles via CV qui sont assemblés (Ensemble) pour une robustesse maximale, sans ré-entraînement global.\n",
                "5. **Evaluation Test & Sauvegarde** : On évalue cet Ensemble final sur le Test set (jamais vu)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\aubin\\Majeur IA\\data analysis\\credit-scoring\\venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
                        "  return FileStore(store_uri, store_uri)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<Experiment: artifact_location=('file:///c:/Users/aubin/Majeur IA/data '\n",
                            " 'analysis/credit-scoring/notebooks/../mlruns/813913874702955039'), creation_time=1765642766148, experiment_id='813913874702955039', last_update_time=1765642766148, lifecycle_stage='active', name='Credit_Scoring_Final_Workflow', tags={'mlflow.experimentKind': 'custom_model_development'}>"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import mlflow\n",
                "import sys\n",
                "import os\n",
                "import joblib\n",
                "import shutil\n",
                "\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
                "if project_root not in sys.path: sys.path.append(project_root)\n",
                "\n",
                "from src.model_utils import (\n",
                "    get_train_val_test_split,\n",
                "    train_dummy, \n",
                "    train_random_forest, \n",
                "    train_xgboost, \n",
                "    train_lightgbm,\n",
                "    train_model_cv, \n",
                "    optimize_lightgbm,\n",
                "    evaluate_model, \n",
                "    find_best_threshold\n",
                ")\n",
                "\n",
                "mlflow.set_tracking_uri(\"../mlruns\")\n",
                "mlflow.set_experiment(\"Credit_Scoring_Final_Workflow\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chargement V1/V2\n",
                "X_v1 = pd.read_pickle('../data/processed/X_prepared_v1.pkl')\n",
                "y_v1 = pd.read_pickle('../data/processed/y_prepared_v1.pkl')\n",
                "X_v2 = pd.read_pickle('../data/processed/X_prepared_v2.pkl')\n",
                "y_v2 = pd.read_pickle('../data/processed/y_prepared_v2.pkl')\n",
                "\n",
                "def clean_cols(df):\n",
                "    df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n",
                "    return df\n",
                "X_v1 = clean_cols(X_v1)\n",
                "X_v2 = clean_cols(X_v2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bench_md",
            "metadata": {},
            "source": [
                "## 1. Entraînement initial"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_bench",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Benchmarking Dataset v1\n",
                        "Entraînement LightGBM_v1...\n",
                        "[LightGBM] [Info] Number of positive: 17377, number of negative: 197880\n",
                        "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035342 seconds.\n",
                        "You can set `force_row_wise=true` to remove the overhead.\n",
                        "And if memory is not enough, you can set `force_col_wise=true`.\n",
                        "[LightGBM] [Info] Total Bins 20520\n",
                        "[LightGBM] [Info] Number of data points in the train set: 215257, number of used features: 275\n",
                        "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
                        "[LightGBM] [Info] Start training from score -0.000000\n",
                        "Training until validation scores don't improve for 50 rounds\n",
                        "[50]\tvalid_0's binary_logloss: 0.581632\tvalid_0's business_cost: 24417\n",
                        "[100]\tvalid_0's binary_logloss: 0.559113\tvalid_0's business_cost: 23567\n",
                        "[150]\tvalid_0's binary_logloss: 0.547297\tvalid_0's business_cost: 23280\n",
                        "[200]\tvalid_0's binary_logloss: 0.538914\tvalid_0's business_cost: 23258\n",
                        "[250]\tvalid_0's binary_logloss: 0.531875\tvalid_0's business_cost: 23227\n",
                        "Early stopping, best iteration is:\n",
                        "[212]\tvalid_0's binary_logloss: 0.53717\tvalid_0's business_cost: 23192\n",
                        "Meilleur seuil trouvé (Val): 0.51 (Coût: 23141)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:06:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
                        "2025/12/13 19:06:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Metrics (Test): {'auc': 0.7818178650645198, 'recall': 0.685016111707841, 'f1': 0.29554538608584835, 'accuracy': 0.7363583150866088, 'business_cost': np.int64(22718), 'val_best_cost': np.int64(23141)}\n",
                        "Benchmarking Dataset v2\n",
                        "Entraînement LightGBM_v2...\n",
                        "[LightGBM] [Info] Number of positive: 17377, number of negative: 197880\n",
                        "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069702 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 12549\n",
                        "[LightGBM] [Info] Number of data points in the train set: 215257, number of used features: 117\n",
                        "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
                        "[LightGBM] [Info] Start training from score -0.000000\n",
                        "Training until validation scores don't improve for 50 rounds\n",
                        "[50]\tvalid_0's binary_logloss: 0.580694\tvalid_0's business_cost: 24346\n",
                        "[100]\tvalid_0's binary_logloss: 0.558407\tvalid_0's business_cost: 23624\n",
                        "[150]\tvalid_0's binary_logloss: 0.546756\tvalid_0's business_cost: 23311\n",
                        "[200]\tvalid_0's binary_logloss: 0.538804\tvalid_0's business_cost: 23135\n",
                        "Early stopping, best iteration is:\n",
                        "[183]\tvalid_0's binary_logloss: 0.541236\tvalid_0's business_cost: 23048\n",
                        "Meilleur seuil trouvé (Val): 0.50 (Coût: 23048)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:07:19 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
                        "2025/12/13 19:07:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Metrics (Test): {'auc': 0.7789777125237858, 'recall': 0.6855531686358755, 'f1': 0.28369818868763197, 'accuracy': 0.7205107637609209, 'business_cost': np.int64(23431), 'val_best_cost': np.int64(23048)}\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Data</th>\n",
                            "      <th>Model</th>\n",
                            "      <th>business_cost</th>\n",
                            "      <th>auc</th>\n",
                            "      <th>val_best_cost</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>v2</td>\n",
                            "      <td>LightGBM</td>\n",
                            "      <td>23431</td>\n",
                            "      <td>0.778978</td>\n",
                            "      <td>23048</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>v1</td>\n",
                            "      <td>LightGBM</td>\n",
                            "      <td>22718</td>\n",
                            "      <td>0.781818</td>\n",
                            "      <td>23141</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "  Data     Model  business_cost       auc  val_best_cost\n",
                            "1   v2  LightGBM          23431  0.778978          23048\n",
                            "0   v1  LightGBM          22718  0.781818          23141"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "results = []\n",
                "\n",
                "# On stocke les données splittées pour pouvoir réutiliser celles du gagnant\n",
                "splits = {}\n",
                "\n",
                "for name, X, y in [(\"v1\", X_v1, y_v1), (\"v2\", X_v2, y_v2)]:\n",
                "    print(f\"Benchmarking Dataset {name}\")\n",
                "    Xt, yt, Xv, yv, Xte, yte = get_train_val_test_split(X, y)\n",
                "    splits[name] = (Xt, yt, Xv, yv, Xte, yte)\n",
                "    \n",
                "    # Dummy\n",
                "    _, m = train_dummy(Xt, yt, Xv, yv, Xte, yte, name)\n",
                "    results.append({\"Data\": name, \"Model\": \"Dummy\", **m})\n",
                "    \n",
                "    # RF\n",
                "    _, m = train_random_forest(Xt, yt, Xv, yv, Xte, yte, name)\n",
                "    results.append({\"Data\": name, \"Model\": \"RF\", **m})\n",
                "    \n",
                "    # XGB\n",
                "    _, m = train_xgboost(Xt, yt, Xv, yv, Xte, yte, name)\n",
                "    results.append({\"Data\": name, \"Model\": \"XGB\", **m})\n",
                "    \n",
                "    # LightGBM (Baseline)\n",
                "    _, m = train_lightgbm(Xt, yt, Xv, yv, Xte, yte, name)\n",
                "    results.append({\"Data\": name, \"Model\": \"LightGBM\", **m})\n",
                "\n",
                "df_res = pd.DataFrame(results).sort_values(\"val_best_cost\")\n",
                "display(df_res[[\"Data\", \"Model\", \"business_cost\", \"auc\", \"val_best_cost\"]])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "select_ds_md",
            "metadata": {},
            "source": [
                "## 2. Sélection du Dataset Gagnant pour l'Optimisation\n",
                "On prend le dataset qui a donné le meilleur score LightGBM (le modèle cible)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "select_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset sélectionné pour optimisation LightGBM : v2\n",
                        "Shape Train: (215257, 124), Val: (46126, 124), Test: (46127, 124)\n"
                    ]
                }
            ],
            "source": [
                "lgbm_res = df_res[df_res[\"Model\"] == \"LightGBM\"].sort_values(\"val_best_cost\")\n",
                "best_data_name = lgbm_res.iloc[0][\"Data\"]\n",
                "print(f\"Dataset sélectionné pour optimisation LightGBM : {best_data_name}\")\n",
                "\n",
                "# Récupération des splits EXISTANTS\n",
                "X_train, y_train, X_val, y_val, X_test, y_test = splits[best_data_name]\n",
                "print(f\"Shape Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "optuna_md",
            "metadata": {},
            "source": [
                "## 3. Optimisation Optuna LightGBM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "run_optuna",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[I 2025-12-13 19:07:36,423] A new study created in memory with name: no-name-a33cd128-8386-47c3-ba34-fb5f3b0bd4b9\n",
                        "[I 2025-12-13 19:08:04,282] Trial 0 finished with value: 23053.0 and parameters: {'learning_rate': 0.20556559861622456, 'num_leaves': 219, 'max_depth': 4, 'min_child_samples': 89, 'min_split_gain': 0.08886159437542485, 'reg_alpha': 31.648969953576, 'reg_lambda': 20.60133891829057}. Best is trial 0 with value: 23053.0.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Meilleurs params: {'learning_rate': 0.20556559861622456, 'num_leaves': 219, 'max_depth': 4, 'min_child_samples': 89, 'min_split_gain': 0.08886159437542485, 'reg_alpha': 31.648969953576, 'reg_lambda': 20.60133891829057}\n"
                    ]
                }
            ],
            "source": [
                "best_params = optimize_lightgbm(X_train, y_train, X_val, y_val, n_trials=10)\n",
                "\n",
                "final_params = best_params.copy()\n",
                "final_params.update({\n",
                "    \"metric\": \"custom\", \"objective\": \"binary\", \"verbosity\": -1,\n",
                "    \"boosting_type\": \"gbdt\", \"random_state\": 42, \"n_jobs\": -1,\n",
                "    \"class_weight\": \"balanced\", \"n_estimators\": 1000\n",
                "})"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cv_ensemble_md",
            "metadata": {},
            "source": [
                "## 4. Cross-Validation & Construction de l'Ensemble Final\n",
                "On entraîne 5 modèles fold par CV. L'Ensemble final est la moyenne de ces 5 modèles (stockée dans une classe simplifiée)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "run_cv_final",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:08:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fold 1 terminé (best_iteration=100).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:08:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
                        "2025/12/13 19:09:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fold 2 terminé (best_iteration=188).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:09:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
                        "2025/12/13 19:13:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fold 3 terminé (best_iteration=214).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:13:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
                        "2025/12/13 19:15:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fold 4 terminé (best_iteration=148).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:15:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fold 5 terminé (best_iteration=176).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:17:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
                        "2025/12/13 19:17:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Calibration seuil Ensemble sur X_val...\n",
                        "Metrics Test (Ensemble): {'auc': 0.7861555974863765, 'recall': 0.680719656283566, 'f1': 0.30399328456649477, 'accuracy': 0.7483469551455764, 'business_cost': np.int64(22309)}\n"
                    ]
                },
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\aubin\\\\Majeur IA\\\\data analysis\\\\credit-scoring\\\\notebooks\\\\models\\\\LGBM_Ensemble_v1_Ensemble_Final.joblib'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Appel à notre fonction CV simplifiée qui retourne l'Ensemble et ses metrics Test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ensemble_final, metrics_final = \u001b[43mtrain_model_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbest_data_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_Ensemble_Final\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal_params\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMETRICS FINALES SUR TEST (Ensemble CV):\u001b[39m\u001b[33m\"\u001b[39m, metrics_final)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aubin\\Majeur IA\\data analysis\\credit-scoring\\src\\model_utils.py:243\u001b[39m, in \u001b[36mtrain_model_cv\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, X_test, y_test, dataset_name, params)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# ENREGISTREMENT DU MODÈLE DE PRODUCTION\u001b[39;00m\n\u001b[32m    242\u001b[39m output_path = os.path.join(os.getcwd(), \u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLGBM_Ensemble_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m mlflow.log_artifact(output_path)\n\u001b[32m    246\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModèle Ensemble final sauvegardé à \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aubin\\Majeur IA\\data analysis\\credit-scoring\\venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:599\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(value, filename, compress, protocol)\u001b[39m\n\u001b[32m    597\u001b[39m         NumpyPickler(f, protocol=protocol).dump(value)\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    600\u001b[39m         NumpyPickler(f, protocol=protocol).dump(value)\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\aubin\\\\Majeur IA\\\\data analysis\\\\credit-scoring\\\\notebooks\\\\models\\\\LGBM_Ensemble_v1_Ensemble_Final.joblib'"
                    ]
                }
            ],
            "source": [
                "# Appel à notre fonction CV simplifiée qui retourne l'Ensemble et ses metrics Test\n",
                "\n",
                "ensemble_final, metrics_final = train_model_cv(\n",
                "    X_train, y_train, X_val, y_val, X_test, y_test, \n",
                "    dataset_name=f\"{best_data_name}_Ensemble_Final\", \n",
                "    params=final_params\n",
                ")\n",
                "\n",
                "print(\"METRICS FINALES SUR TEST (Ensemble CV):\", metrics_final)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Entraînement du modèle final sans CV car le notebook 3 ne fonctionne pas avec l'ensemble de modèles du CV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Entraînement LightGBM_v1_Ensemble_Final...\n",
                        "Training until validation scores don't improve for 50 rounds\n",
                        "[50]\tvalid_0's business_cost: 23666\n",
                        "[100]\tvalid_0's business_cost: 23263\n",
                        "[150]\tvalid_0's business_cost: 23220\n",
                        "Early stopping, best iteration is:\n",
                        "[116]\tvalid_0's business_cost: 23053\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:39:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Meilleur seuil trouvé (Val): 0.50 (Coût: 23053)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/12/13 19:39:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Metrics (Test): {'auc': 0.7828163973056544, 'recall': 0.6987110633727175, 'f1': 0.29283664397051373, 'accuracy': 0.7275565287142021, 'business_cost': np.int64(22665), 'val_best_cost': np.int64(23053)}\n"
                    ]
                }
            ],
            "source": [
                "#on repasse au meilleur modele avec meilleur parametre sans CV car ne fonctionne pas\n",
                "modele_final, metrics_final = train_lightgbm(\n",
                "    X_train, y_train, X_val, y_val, X_test, y_test, \n",
                "    dataset_name=f\"{best_data_name}_Ensemble_Final\", \n",
                "    params=final_params\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_md",
            "metadata": {},
            "source": [
                "## 6. Sauvegarde"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "save_disk",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Modèle Ensemble sauvegardé et prêt pour Docker!\n"
                    ]
                }
            ],
            "source": [
                "if not os.path.exists(\"../models\"):\n",
                "    os.makedirs(\"../models\")\n",
                "\n",
                "joblib.dump(modele_final, \"../models/best_model.pkl\")\n",
                "\n",
                "path_serving = \"../models/final_model\"\n",
                "if os.path.exists(path_serving): shutil.rmtree(path_serving)\n",
                "# Sauvegarde MLflow comme modèle sklearn\n",
                "mlflow.sklearn.save_model(modele_final, path_serving)\n",
                "print(\"Modèle Ensemble sauvegardé et prêt pour Docker!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
