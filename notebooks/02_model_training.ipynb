{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1326f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aubin\\Majeur IA\\data analysis\\credit-scoring\\venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/12/05 08:53:47 INFO mlflow.tracking.fluent: Experiment with name 'Credit_Scoring_Exp' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow initialisé !\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, make_scorer\n",
    "\n",
    "# Configuration de MLflow\n",
    "# On donne un nom à l'expérience pour s'y retrouver\n",
    "mlflow.set_experiment(\"Credit_Scoring_Exp\")\n",
    "\n",
    "print(\"MLflow initialisé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a759887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (246008, 281), Test : (61503, 281)\n"
     ]
    }
   ],
   "source": [
    "# On charge les pickles qu'on a sauvegardés à la fin du notebook 01\n",
    "X_train = pd.read_pickle('../data/X_train.pkl')\n",
    "X_test = pd.read_pickle('../data/X_test.pkl')\n",
    "y_train = pd.read_pickle('../data/y_train.pkl')\n",
    "y_test = pd.read_pickle('../data/y_test.pkl')\n",
    "\n",
    "print(f\"Train : {X_train.shape}, Test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c7d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_cost_metric(y_true, y_pred_proba, threshold=0.5):\n",
    "\n",
    "    # Convertir probas en classes 0/1 selon le seuil\n",
    "    y_pred = (y_pred_proba > threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Formule du coût\n",
    "    cost = 10 * fn + 1 * fp\n",
    "    \n",
    "    # On normalise pour avoir un score comparable (optionnel, mais pratique)\n",
    "    # Plus c'est bas, mieux c'est.\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "604bf5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/05 10:46:04 INFO mlflow.tracking.fluent: Autologging successfully enabled for lightgbm.\n",
      "2025/12/05 10:46:10 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/12/05 10:46:12 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '1a5a4732b05d487798aa2473d612d962', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2025/12/05 10:46:27 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\aubin\\Majeur IA\\data analysis\\credit-scoring\\venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "c:\\Users\\aubin\\Majeur IA\\data analysis\\credit-scoring\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "2025/12/05 10:46:47 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\aubin\\Majeur IA\\data analysis\\credit-scoring\\venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2025/12/05 10:46:48 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy AUC: 0.5000\n",
      "Dummy Cost: 49650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/05 10:46:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run logged to MLflow!\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog()\n",
    "# 1. Création du modèle \"Idiot\" (Stratégie: prédit la classe majoritaire)\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# 2. Entraînement\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# 3. Prédictions (Probas)\n",
    "# Le dummy va mettre 0 partout ou des probas basiques\n",
    "y_prob = dummy.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 4. Calcul des Métriques\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "cost_score = business_cost_metric(y_test, y_prob)\n",
    "\n",
    "print(f\"Dummy AUC: {auc_score:.4f}\")     # Devrait être 0.5\n",
    "print(f\"Dummy Cost: {cost_score}\")       # Sera élevé car il rate tous les défauts (FN)\n",
    "\n",
    "# 5. LOGGING MLFLOW (C'est ça qu'on veut !)\n",
    "# On enregistre les paramètres (quel algo ?)\n",
    "mlflow.log_param(\"model_type\", \"DummyClassifier\")\n",
    "mlflow.log_param(\"strategy\", \"most_frequent\")\n",
    "\n",
    "# On enregistre les performances\n",
    "mlflow.log_metric(\"auc\", auc_score)\n",
    "mlflow.log_metric(\"business_cost\", cost_score)\n",
    "\n",
    "# On enregistre le modèle lui-même (le fichier)\n",
    "mlflow.sklearn.log_model(dummy, \"model\")\n",
    "\n",
    "print(\"Run logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cafd59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des caractères spéciaux dans les noms de colonnes (pour LightGBM)\n",
    "X_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "X_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aaadbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement LightGBM en cours...\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16615\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 260\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "LGBM AUC: 0.78\n",
      "LGBM Cost: 31090\n",
      "Training Duration: 15.46 seconds\n"
     ]
    }
   ],
   "source": [
    "# modele LightGBM avec gestion du déséquilibre sur le dataset v1\n",
    "import lightgbm as lgb\n",
    "\n",
    "start_time = pd.Timestamp.now()\n",
    "# class_weight='balanced' gère automatiquement le déséquilibre 92/8\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Entraînement LightGBM en cours...\")\n",
    "lgbm.fit(X_train, y_train)\n",
    "end_time = pd.Timestamp.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "y_prob = lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "cost_score = business_cost_metric(y_test, y_prob)\n",
    "\n",
    "print(f\"LGBM AUC: {auc_score:.2f}\")   \n",
    "print(f\"LGBM Cost: {cost_score}\")    \n",
    "print(f\"Training Duration: {duration:.2f} seconds\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83dc83b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM AUC: 0.7789\n",
      "LGBM Cost: 31090\n"
     ]
    }
   ],
   "source": [
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "cost_score = business_cost_metric(y_test, y_prob)\n",
    "print(f\"LGBM AUC: {auc_score:.4f}\")\n",
    "print(f\"LGBM Cost: {cost_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6368c117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196806, 281)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on recoupe le train pour faire un jeu de validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edc6379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24868\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 245\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.820473\ttraining's binary_logloss: 0.531943\tvalid_1's auc: 0.769292\tvalid_1's binary_logloss: 0.552079\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1000]\ttraining's auc: 0.855057\ttraining's binary_logloss: 0.490501\tvalid_1's auc: 0.7762\tvalid_1's binary_logloss: 0.524969\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1500]\ttraining's auc: 0.879841\ttraining's binary_logloss: 0.461497\tvalid_1's auc: 0.777314\tvalid_1's binary_logloss: 0.506896\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2000]\ttraining's auc: 0.900139\ttraining's binary_logloss: 0.436477\tvalid_1's auc: 0.777828\tvalid_1's binary_logloss: 0.490739\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2500]\ttraining's auc: 0.916716\ttraining's binary_logloss: 0.414146\tvalid_1's auc: 0.77774\tvalid_1's binary_logloss: 0.476264\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[2223]\ttraining's auc: 0.907969\ttraining's binary_logloss: 0.426192\tvalid_1's auc: 0.77792\tvalid_1's binary_logloss: 0.484084\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import log_evaluation, early_stopping\n",
    "start_time = pd.Timestamp.now()\n",
    "\n",
    "clf = lgb.LGBMClassifier(nthread=-1,\n",
    "                            n_estimators=5000,\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=11,\n",
    "                            num_leaves=58,\n",
    "                            colsample_bytree=0.613,\n",
    "                            subsample=0.708,\n",
    "                            max_bin=407,\n",
    "                            reg_alpha=3.564,\n",
    "                            reg_lambda=4.930,\n",
    "                            min_child_weight=6,\n",
    "                            min_child_samples=165,\n",
    "                            class_weight='balanced'\n",
    "                            )\n",
    "\n",
    "clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='auc', callbacks=[\n",
    "        log_evaluation(500),    # Remplace verbose=500\n",
    "        early_stopping(500)     # Remplace early_stopping_rounds=500\n",
    "    ])\n",
    "end_time = pd.Timestamp.now()\n",
    "duration = (end_time - start_time).total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6980bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM AUC: 0.7816\n",
      "LGBM Cost: 30720\n",
      "Training Duration: 96.23 seconds\n"
     ]
    }
   ],
   "source": [
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "cost_score = business_cost_metric(y_test, y_prob)\n",
    "\n",
    "print(f\"LGBM AUC: {auc_score:.4f}\")   \n",
    "print(f\"LGBM Cost: {cost_score}\")    \n",
    "print(f\"Training Duration: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b8a3e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215257, 84)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v2 = pd.read_pickle('../data/train_v2.pkl')\n",
    "X2 = df_v2.drop(columns=['TARGET'])\n",
    "y2 = df_v2['TARGET']\n",
    "#split en 70/15/15\n",
    "X2_train, X2_temp, y2_train, y2_temp = train_test_split(X2, y2, test_size=0.3, random_state=42, stratify=y2)\n",
    "X2_val, X2_test, y2_val, y2_test = train_test_split(X2_temp, y2_temp, test_size=0.5, random_state=42, stratify=y2_temp)\n",
    "X2_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7053ba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17377, number of negative: 197880\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16810\n",
      "[LightGBM] [Info] Number of data points in the train set: 215257, number of used features: 83\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.816299\ttraining's binary_logloss: 0.533125\tvalid_1's auc: 0.774139\tvalid_1's binary_logloss: 0.547687\n",
      "[1000]\ttraining's auc: 0.849534\ttraining's binary_logloss: 0.494404\tvalid_1's auc: 0.779361\tvalid_1's binary_logloss: 0.521827\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1500]\ttraining's auc: 0.873485\ttraining's binary_logloss: 0.466765\tvalid_1's auc: 0.780327\tvalid_1's binary_logloss: 0.504176\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[1473]\ttraining's auc: 0.872322\ttraining's binary_logloss: 0.468115\tvalid_1's auc: 0.780365\tvalid_1's binary_logloss: 0.505043\n"
     ]
    }
   ],
   "source": [
    "# test avec le dataset v2\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "start_time = pd.Timestamp.now()\n",
    "\n",
    "clf = lgb.LGBMClassifier(nthread=-1,\n",
    "                            n_estimators=5000,\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=11,\n",
    "                            num_leaves=58,\n",
    "                            colsample_bytree=0.613,\n",
    "                            subsample=0.708,\n",
    "                            max_bin=407,\n",
    "                            reg_alpha=3.564,\n",
    "                            reg_lambda=4.930,\n",
    "                            min_child_weight=6,\n",
    "                            min_child_samples=165,\n",
    "                            class_weight='balanced'\n",
    "                            )\n",
    "\n",
    "clf.fit(X2_train, y2_train, eval_set=[(X2_train, y2_train), (X2_val, y2_val)], eval_metric='auc', callbacks=[\n",
    "        log_evaluation(500),    # Remplace verbose=500\n",
    "        early_stopping(500)     # Remplace early_stopping_rounds=500\n",
    "    ])\n",
    "end_time = pd.Timestamp.now()\n",
    "duration = (end_time - start_time).total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2063639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM AUC: 0.7814\n",
      "LGBM Cost: 23254\n",
      "Training Duration: 60.04 seconds\n"
     ]
    }
   ],
   "source": [
    "y2_prob = clf.predict_proba(X2_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y2_test, y2_prob)\n",
    "cost_score = business_cost_metric(y2_test, y2_prob)\n",
    "\n",
    "print(f\"LGBM AUC: {auc_score:.4f}\")   \n",
    "print(f\"LGBM Cost: {cost_score}\")    \n",
    "print(f\"Training Duration: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "733ed87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 09:45:28,956] A new study created in memory with name: no-name-37863346-0df6-4a2e-9d14-f35990440a0f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche des meilleurs hyperparamètres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 09:45:35,588] Trial 0 finished with value: 0.7789117706254458 and parameters: {'n_estimators': 3957, 'learning_rate': 0.04676931605058948, 'num_leaves': 43, 'max_depth': 8, 'min_child_samples': 44, 'reg_alpha': 9.61608283355706, 'reg_lambda': 5.188404512324313, 'colsample_bytree': 0.7715562991397976, 'subsample': 0.9371417105583603}. Best is trial 0 with value: 0.7789117706254458.\n",
      "[I 2025-12-05 09:45:44,445] Trial 1 finished with value: 0.7756934727497279 and parameters: {'n_estimators': 750, 'learning_rate': 0.01540322686298918, 'num_leaves': 34, 'max_depth': 5, 'min_child_samples': 43, 'reg_alpha': 5.962406285776504, 'reg_lambda': 3.8244617278626714, 'colsample_bytree': 0.7596775866992767, 'subsample': 0.6872535225080543}. Best is trial 0 with value: 0.7789117706254458.\n",
      "[I 2025-12-05 09:46:19,233] Trial 2 finished with value: 0.7739015711446432 and parameters: {'n_estimators': 1151, 'learning_rate': 0.005107373790345892, 'num_leaves': 46, 'max_depth': 9, 'min_child_samples': 20, 'reg_alpha': 7.097233237381859, 'reg_lambda': 2.8155225696966015, 'colsample_bytree': 0.7576123171942909, 'subsample': 0.8238060331113004}. Best is trial 0 with value: 0.7789117706254458.\n",
      "[I 2025-12-05 09:46:33,303] Trial 3 finished with value: 0.7797653976573059 and parameters: {'n_estimators': 2891, 'learning_rate': 0.03650008862742155, 'num_leaves': 23, 'max_depth': 12, 'min_child_samples': 93, 'reg_alpha': 9.69854414873663, 'reg_lambda': 9.336644156532365, 'colsample_bytree': 0.7202662212910708, 'subsample': 0.9390368713207532}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:46:48,815] Trial 4 finished with value: 0.77555415238568 and parameters: {'n_estimators': 811, 'learning_rate': 0.009803131156190708, 'num_leaves': 67, 'max_depth': 7, 'min_child_samples': 92, 'reg_alpha': 1.9610097504808899, 'reg_lambda': 0.3430718724654158, 'colsample_bytree': 0.8376348960736505, 'subsample': 0.8309504595071202}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:46:53,314] Trial 5 finished with value: 0.7767774528341751 and parameters: {'n_estimators': 2597, 'learning_rate': 0.09203218383388533, 'num_leaves': 80, 'max_depth': 11, 'min_child_samples': 68, 'reg_alpha': 9.480445749611798, 'reg_lambda': 3.253936605035319, 'colsample_bytree': 0.7712636812761232, 'subsample': 0.7109608385686051}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:47:07,349] Trial 6 finished with value: 0.7792666973926538 and parameters: {'n_estimators': 2356, 'learning_rate': 0.022870449027484512, 'num_leaves': 86, 'max_depth': 8, 'min_child_samples': 98, 'reg_alpha': 6.277741616048543, 'reg_lambda': 7.816004223986415, 'colsample_bytree': 0.7702296167681315, 'subsample': 0.7225304573442541}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:47:16,416] Trial 7 finished with value: 0.7790819778720571 and parameters: {'n_estimators': 2761, 'learning_rate': 0.048554496271811395, 'num_leaves': 21, 'max_depth': 4, 'min_child_samples': 43, 'reg_alpha': 8.960649368806664, 'reg_lambda': 0.959205104569465, 'colsample_bytree': 0.5990070115306331, 'subsample': 0.4319343157342028}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:47:36,156] Trial 8 finished with value: 0.7795445843186525 and parameters: {'n_estimators': 4273, 'learning_rate': 0.03825040748792077, 'num_leaves': 48, 'max_depth': 3, 'min_child_samples': 96, 'reg_alpha': 5.717505900740935, 'reg_lambda': 1.716987905856573, 'colsample_bytree': 0.5787841334355267, 'subsample': 0.5259912584517972}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:47:51,029] Trial 9 finished with value: 0.7791959983573196 and parameters: {'n_estimators': 2924, 'learning_rate': 0.020767680634144206, 'num_leaves': 95, 'max_depth': 5, 'min_child_samples': 50, 'reg_alpha': 6.234303055614905, 'reg_lambda': 1.7724327298217002, 'colsample_bytree': 0.6869577200351122, 'subsample': 0.6335687306409522}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:47:57,772] Trial 10 finished with value: 0.7779749276427583 and parameters: {'n_estimators': 3575, 'learning_rate': 0.09284955670505839, 'num_leaves': 20, 'max_depth': 12, 'min_child_samples': 74, 'reg_alpha': 2.947640990168838, 'reg_lambda': 9.537397600097151, 'colsample_bytree': 0.416335119813217, 'subsample': 0.9930882938160472}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:48:13,811] Trial 11 finished with value: 0.779127098311864 and parameters: {'n_estimators': 4916, 'learning_rate': 0.040211367405322815, 'num_leaves': 58, 'max_depth': 3, 'min_child_samples': 82, 'reg_alpha': 3.982802474386821, 'reg_lambda': 6.134068287376454, 'colsample_bytree': 0.9625821685506466, 'subsample': 0.4904144671503399}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:48:23,509] Trial 12 finished with value: 0.7786437338024795 and parameters: {'n_estimators': 4748, 'learning_rate': 0.034100026637916094, 'num_leaves': 35, 'max_depth': 10, 'min_child_samples': 98, 'reg_alpha': 0.3050429883580792, 'reg_lambda': 9.827094983258277, 'colsample_bytree': 0.5286435466213578, 'subsample': 0.5525028726104765}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:48:30,401] Trial 13 finished with value: 0.779049146306773 and parameters: {'n_estimators': 1763, 'learning_rate': 0.0599152321778966, 'num_leaves': 59, 'max_depth': 6, 'min_child_samples': 65, 'reg_alpha': 7.90719047972836, 'reg_lambda': 6.634503461849328, 'colsample_bytree': 0.6157829778855483, 'subsample': 0.8457661564046477}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:49:06,811] Trial 14 finished with value: 0.7795823250992743 and parameters: {'n_estimators': 3565, 'learning_rate': 0.026771797737484858, 'num_leaves': 31, 'max_depth': 3, 'min_child_samples': 85, 'reg_alpha': 4.737518704625214, 'reg_lambda': 8.55632391022416, 'colsample_bytree': 0.5070864053145987, 'subsample': 0.5845215477504434}. Best is trial 3 with value: 0.7797653976573059.\n",
      "[I 2025-12-05 09:49:29,769] Trial 15 finished with value: 0.7808532987795519 and parameters: {'n_estimators': 3396, 'learning_rate': 0.02554583292637762, 'num_leaves': 31, 'max_depth': 12, 'min_child_samples': 82, 'reg_alpha': 4.411447783995156, 'reg_lambda': 8.33974067909749, 'colsample_bytree': 0.4116836729487334, 'subsample': 0.6047869532601601}. Best is trial 15 with value: 0.7808532987795519.\n",
      "[I 2025-12-05 09:50:07,047] Trial 16 finished with value: 0.7803289755780726 and parameters: {'n_estimators': 3272, 'learning_rate': 0.014225227681777287, 'num_leaves': 27, 'max_depth': 12, 'min_child_samples': 80, 'reg_alpha': 3.398408822370545, 'reg_lambda': 8.016432405355765, 'colsample_bytree': 0.40876397618537846, 'subsample': 0.7689174448044527}. Best is trial 15 with value: 0.7808532987795519.\n",
      "[I 2025-12-05 09:50:45,662] Trial 17 finished with value: 0.7805180658839376 and parameters: {'n_estimators': 1997, 'learning_rate': 0.012278517612616861, 'num_leaves': 30, 'max_depth': 10, 'min_child_samples': 59, 'reg_alpha': 2.8381957326595804, 'reg_lambda': 7.736100827940385, 'colsample_bytree': 0.41085723836964816, 'subsample': 0.7899049913716909}. Best is trial 15 with value: 0.7808532987795519.\n",
      "[I 2025-12-05 09:51:37,107] Trial 18 finished with value: 0.7799517704749388 and parameters: {'n_estimators': 2018, 'learning_rate': 0.007450236788488737, 'num_leaves': 40, 'max_depth': 10, 'min_child_samples': 29, 'reg_alpha': 1.662852654848563, 'reg_lambda': 7.025735539379127, 'colsample_bytree': 0.4792663157543917, 'subsample': 0.6391127000725771}. Best is trial 15 with value: 0.7808532987795519.\n",
      "[I 2025-12-05 09:52:19,989] Trial 19 finished with value: 0.7811314327499354 and parameters: {'n_estimators': 1658, 'learning_rate': 0.012535077735683063, 'num_leaves': 70, 'max_depth': 10, 'min_child_samples': 59, 'reg_alpha': 4.623067356388232, 'reg_lambda': 5.412850454194109, 'colsample_bytree': 0.4521696666688364, 'subsample': 0.4000189134569926}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 09:52:52,053] Trial 20 finished with value: 0.7800359936067194 and parameters: {'n_estimators': 1582, 'learning_rate': 0.0181941223348105, 'num_leaves': 71, 'max_depth': 11, 'min_child_samples': 57, 'reg_alpha': 4.8576510412463625, 'reg_lambda': 5.172158460116274, 'colsample_bytree': 0.46671892694633804, 'subsample': 0.43355388182393817}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 09:53:38,275] Trial 21 finished with value: 0.7806418921309326 and parameters: {'n_estimators': 2235, 'learning_rate': 0.011180883865732185, 'num_leaves': 54, 'max_depth': 10, 'min_child_samples': 57, 'reg_alpha': 2.343049756966885, 'reg_lambda': 6.026514182488282, 'colsample_bytree': 0.4029729445471736, 'subsample': 0.40846655867298437}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 09:54:15,668] Trial 22 finished with value: 0.7795533828994318 and parameters: {'n_estimators': 1237, 'learning_rate': 0.009472412815995045, 'num_leaves': 52, 'max_depth': 9, 'min_child_samples': 32, 'reg_alpha': 1.6329492571156492, 'reg_lambda': 4.478811268205463, 'colsample_bytree': 0.45975905302147146, 'subsample': 0.4068405870387499}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 09:55:58,617] Trial 23 finished with value: 0.7807730790336984 and parameters: {'n_estimators': 2426, 'learning_rate': 0.006360696885854658, 'num_leaves': 68, 'max_depth': 11, 'min_child_samples': 70, 'reg_alpha': 3.8721985683912896, 'reg_lambda': 6.438972997740766, 'colsample_bytree': 0.5567775772232181, 'subsample': 0.4794850946608151}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 09:57:37,896] Trial 24 finished with value: 0.7807265903480685 and parameters: {'n_estimators': 3292, 'learning_rate': 0.0050059031176404355, 'num_leaves': 68, 'max_depth': 11, 'min_child_samples': 71, 'reg_alpha': 4.134170449013076, 'reg_lambda': 6.912563722118033, 'colsample_bytree': 0.5506190386416311, 'subsample': 0.4929791900879743}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 09:58:19,092] Trial 25 finished with value: 0.7797797832418631 and parameters: {'n_estimators': 1439, 'learning_rate': 0.00710232074344294, 'num_leaves': 77, 'max_depth': 11, 'min_child_samples': 77, 'reg_alpha': 3.9815366550967988, 'reg_lambda': 5.730382089804109, 'colsample_bytree': 0.6435969199434229, 'subsample': 0.4746118577481204}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 09:59:24,345] Trial 26 finished with value: 0.7805824304478681 and parameters: {'n_estimators': 2460, 'learning_rate': 0.0072965995414369, 'num_leaves': 63, 'max_depth': 9, 'min_child_samples': 65, 'reg_alpha': 5.420546493824347, 'reg_lambda': 8.554379211989634, 'colsample_bytree': 0.5020471127814393, 'subsample': 0.583549880359608}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 09:59:46,171] Trial 27 finished with value: 0.7805838557039341 and parameters: {'n_estimators': 4168, 'learning_rate': 0.027948063415013987, 'num_leaves': 75, 'max_depth': 12, 'min_child_samples': 87, 'reg_alpha': 7.224042542220198, 'reg_lambda': 4.205480949584647, 'colsample_bytree': 0.5528183613756135, 'subsample': 0.4675463835788032}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 10:00:11,087] Trial 28 finished with value: 0.7795395547483581 and parameters: {'n_estimators': 3251, 'learning_rate': 0.01685484576946552, 'num_leaves': 90, 'max_depth': 11, 'min_child_samples': 10, 'reg_alpha': 0.36320916460852537, 'reg_lambda': 7.183241819603558, 'colsample_bytree': 0.4632838920498879, 'subsample': 0.5451152359326955}. Best is trial 19 with value: 0.7811314327499354.\n",
      "[I 2025-12-05 10:01:04,615] Trial 29 finished with value: 0.7796781909894944 and parameters: {'n_estimators': 1853, 'learning_rate': 0.005870418754302982, 'num_leaves': 81, 'max_depth': 8, 'min_child_samples': 50, 'reg_alpha': 4.316744493800363, 'reg_lambda': 5.607464846597526, 'colsample_bytree': 0.44257214530819333, 'subsample': 0.624365870467878}. Best is trial 19 with value: 0.7811314327499354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Meilleure AUC trouvée : 0.7811\n",
      "Meilleurs paramètres :\n",
      "{'n_estimators': 1658, 'learning_rate': 0.012535077735683063, 'num_leaves': 70, 'max_depth': 10, 'min_child_samples': 59, 'reg_alpha': 4.623067356388232, 'reg_lambda': 5.412850454194109, 'colsample_bytree': 0.4521696666688364, 'subsample': 0.4000189134569926}\n"
     ]
    }
   ],
   "source": [
    "# optimisation avec optuna sur le v2\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Définition de l'espace de recherche (Hyperparameters)\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced', # Gère le déséquilibre\n",
    "        'n_jobs': -1,\n",
    "        \n",
    "        # Paramètres qu'Optuna va faire varier\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0)\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    \n",
    "    callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "    \n",
    "    model.fit(\n",
    "        X2_train, y2_train,\n",
    "        eval_set=[(X2_val, y2_val)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # On prédit sur le set de VALIDATION pour qu'Optuna juge la qualité\n",
    "    preds = model.predict_proba(X2_val)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y2_val, preds)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "# 4. Lancement de l'étude\n",
    "print(\"Recherche des meilleurs hyperparamètres...\")\n",
    "study = optuna.create_study(direction='maximize') \n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Meilleure AUC trouvée : {study.best_value:.4f}\")\n",
    "print(\"Meilleurs paramètres :\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9343ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
